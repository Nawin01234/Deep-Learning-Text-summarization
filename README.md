# Deep Learning Text Summarization

CNN/DailyMail text summarization ‡πÅ‡∏•‡∏∞ headline generation ‡∏î‡πâ‡∏ß‡∏¢ BERT Encoder-Decoder

## üìã ‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£

‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• 2 ‡∏ï‡∏±‡∏ß‡∏à‡∏≤‡∏Å CNN/DailyMail dataset (287k samples):
1. **Summarization Model** - ‡πÄ‡∏ó‡∏£‡∏ô‡∏à‡∏≤‡∏Å‡∏®‡∏π‡∏ô‡∏¢‡πå (from scratch) 10 epochs
2. **Headline Generation Model** - ‡πÉ‡∏ä‡πâ pretrained BERT 5 epochs

## üéØ ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå

‡∏á‡∏≤‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à:
- ‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• Seq2Seq ‡∏à‡∏≤‡∏Å‡∏®‡∏π‡∏ô‡∏¢‡πå
- ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á from-scratch vs pretrained models
- ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ hallucination ‡πÉ‡∏ô text generation
- ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ö checkpoint ‡πÅ‡∏•‡∏∞ crash recovery

## üèóÔ∏è ‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏°

- **Encoder**: BERT-base-uncased (12 layers, 768 hidden)
- **Decoder**: BERT-base-uncased (12 layers, 768 hidden)
- **Parameters**: ~247M parameters
- **Training**: Mixed precision (FP16) with gradient accumulation

## üìä ‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô

### Summarization Model (from scratch)
- Epochs: 10 (8 initial + 2 fine-tuning)
- Learning Rate: 3e-5 ‚Üí 1.5e-5
- Batch Size: 4 √ó 4 (gradient accumulation)
- Time: ~2 hours/epoch on RTX 4080

### Headline Model (pretrained)
- Epochs: 5
- Learning Rate: 3e-5
- Pretrained: BERT-base encoder & decoder

## üîß Anti-Hallucination Parameters

```python
repetition_penalty=2.0
length_penalty=1.0
no_repeat_ngram_size=3
num_beams=5
```

## üìà ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå

### Summarization (from scratch)
- ‚ùå ‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ hallucination ‡∏™‡∏π‡∏á
- ‚ö†Ô∏è Dataset 287k ‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• 247M params
- üìä ‡πÅ‡∏™‡∏î‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡∏à‡∏≤‡∏Å‡∏®‡∏π‡∏ô‡∏¢‡πå

### Headline Generation (pretrained)
- ‚úÖ ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏î‡∏µ ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ
- ‚úÖ Pretrained knowledge ‡∏ä‡πà‡∏ß‡∏¢‡∏°‡∏≤‡∏Å

## üöÄ ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô

1. ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á dependencies:
```bash
pip install torch transformers datasets rouge-score
```

2. ‡πÄ‡∏õ‡∏¥‡∏î `workshop3.ipynb` ‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏ô‡∏ó‡∏µ‡∏•‡∏∞ cell ‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö

3. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö checkpoints ‡πÉ‡∏ô `checkpoints_new/`

## üìÅ ‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç

- `workshop3.ipynb` - Notebook ‡∏´‡∏•‡∏±‡∏Å (18 cells)
- `my_tokenizer_287k.json` - Custom tokenizer
- `.gitignore` - ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏≠‡∏±‡∏û‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà

## üí° ‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô

1. **Dataset matters**: 287k samples ‡πÑ‡∏°‡πà‡∏û‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö from-scratch training
2. **Pretrained wins**: Pretrained models ‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡∏°‡∏≤‡∏Å
3. **Hallucination is hard**: ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ generation parameters ‡πÅ‡∏•‡∏∞ data ‡πÄ‡∏¢‡∏≠‡∏∞
4. **Checkpoints are crucial**: ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏£‡∏∞‡∏ö‡∏ö‡∏Å‡∏π‡πâ‡∏Ñ‡∏∑‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏°‡∏î‡∏±‡∏ö

## üéì ‡∏™‡∏£‡∏∏‡∏õ

‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏á‡∏≤‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏î‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô:
- ‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡∏à‡∏≤‡∏Å‡∏®‡∏π‡∏ô‡∏¢‡πå
- ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á pretrained models
- ‡∏ß‡∏¥‡∏ò‡∏µ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô (crash, hallucination)

‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ pretrained models ‡πÄ‡∏™‡∏°‡∏≠!

## üìù License

Educational project - free to use and learn from

## üë§ Author

Nawin01234

---

‚≠ê ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå ‡∏≠‡∏¢‡πà‡∏≤‡∏•‡∏∑‡∏° star repo ‡∏ô‡∏µ‡πâ!
